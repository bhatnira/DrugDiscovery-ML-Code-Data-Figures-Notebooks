import streamlit as st
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Set backend before importing pyplot
import matplotlib.pyplot as plt
import seaborn as sns
import os
import shutil
import zipfile
import io
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import SimilarityMaps
import deepchem as dc
from deepchem.feat import ConvMolFeaturizer
from deepchem.models import GraphConvModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import tensorflow as tf
import time
import threading
import queue

# Configure matplotlib and RDKit for headless mode
os.environ['MPLBACKEND'] = 'Agg'
if 'DISPLAY' not in os.environ:
    os.environ['DISPLAY'] = ':99'

# Disable RDKit warnings and configure for headless rendering
from rdkit import RDLogger
RDLogger.DisableLog('rdApp.*')

# Set random seed for reproducibility
tf.random.set_seed(42)

# Function to standardize Smile using RDKit
def standardize_smiles(smiles, verbose=False):
    if verbose:
        st.write(smiles)
    std_mol = standardize_mol(Chem.MolFromSmiles(smiles), verbose=verbose)
    return Chem.MolToSmiles(std_mol)

# Function to standardize molecule using RDKit
def standardize_mol(mol, verbose=False):
    from rdkit.Chem.MolStandardize import rdMolStandardize
    
    clean_mol = rdMolStandardize.Cleanup(mol)
    if verbose:
        st.write('Remove Hs, disconnect metal atoms, normalize the molecule, reionize the molecule:')

    parent_clean_mol = rdMolStandardize.FragmentParent(clean_mol)
    if verbose:
        st.write('Select the "parent" fragment:')

    uncharger = rdMolStandardize.Uncharger()
    uncharged_parent_clean_mol = uncharger.uncharge(parent_clean_mol)
    if verbose:
        st.write('Neutralize the molecule:')

    te = rdMolStandardize.TautomerEnumerator()
    taut_uncharged_parent_clean_mol = te.Canonicalize(uncharged_parent_clean_mol)
    if verbose:
        st.write('Enumerate tautomers:')

    assert taut_uncharged_parent_clean_mol is not None
    if verbose:
        st.write(Chem.MolToSmiles(taut_uncharged_parent_clean_mol))

    return taut_uncharged_parent_clean_mol

# Configure Streamlit page for mobile-friendly display
# st.set_page_config(
#     page_title="GraphConv Regressor",
#     page_icon="üß¨",
#     layout="wide",
#     initial_sidebar_state="collapsed"
# )

# Apple-style iOS interface CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=SF+Pro+Display:wght@300;400;500;600;700&display=swap');
    
    /* Global iOS-like styling */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', 'Segoe UI', 'Roboto', sans-serif;
    }
    
    /* Main container */
    .main .block-container {
        max-width: 100%;
        padding: 1rem;
        background: transparent;
    }
    
    /* Apple-style buttons */
    .stButton > button {
        background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);
        color: white;
        border-radius: 12px;
        border: none;
        padding: 14px 20px;
        font-weight: 600;
        font-size: 16px;
        transition: all 0.2s ease;
        width: 100%;
        box-shadow: 0 4px 16px rgba(255, 107, 107, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(255, 107, 107, 0.4);
        background: linear-gradient(135deg, #FF5252 0%, #26A69A 100%);
    }
    
    .stButton > button:active {
        transform: translateY(0px);
        box-shadow: 0 2px 8px rgba(255, 107, 107, 0.3);
    }
    
    /* iOS Input fields */
    .stTextInput > div > div > input {
        background: rgba(255, 255, 255, 0.8);
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border-radius: 12px;
        border: 1px solid rgba(0, 0, 0, 0.1);
        padding: 16px;
        font-size: 16px;
        transition: all 0.2s ease;
    }
    
    /* Fallback for input fields */
    @supports not (backdrop-filter: blur(10px)) {
        .stTextInput > div > div > input {
            background: rgba(255, 255, 255, 0.95);
        }
    }
    
    .stTextInput > div > div > input:focus {
        border: 2px solid #FF6B6B;
        box-shadow: 0 0 0 4px rgba(255, 107, 107, 0.1);
        outline: none;
    }
    
    .stSelectbox > div > div > select {
        background: rgba(255, 255, 255, 0.8);
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border-radius: 12px;
        border: 1px solid rgba(0, 0, 0, 0.1);
        padding: 16px;
        font-size: 16px;
    }
    
    /* Fallback for select boxes */
    @supports not (backdrop-filter: blur(10px)) {
        .stSelectbox > div > div > select {
            background: rgba(255, 255, 255, 0.95);
        }
    }
    
    /* iOS Progress bar */
    .stProgress > div > div > div {
        background: linear-gradient(90deg, #FF6B6B 0%, #4ECDC4 100%);
        border-radius: 8px;
        height: 8px;
    }
    
    .stProgress > div > div {
        background: rgba(0, 0, 0, 0.1);
        border-radius: 8px;
        height: 8px;
    }
    
    /* File uploader - iOS style */
    .stFileUploader > div {
        background: rgba(255, 255, 255, 0.8);
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border-radius: 16px;
        border: 2px dashed #FF6B6B;
        padding: 32px;
        text-align: center;
        transition: all 0.3s ease;
    }
    
    /* Fallback for file uploader */
    @supports not (backdrop-filter: blur(10px)) {
        .stFileUploader > div {
            background: rgba(255, 255, 255, 0.95);
        }
    }
    
    .stFileUploader > div:hover {
        background: rgba(255, 107, 107, 0.05);
        border-color: #4ECDC4;
    }
    
    /* Tabs - iOS style */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background: rgba(255, 255, 255, 0.8);
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border-radius: 16px;
        padding: 8px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
    }
    
    /* Fallback for tabs */
    @supports not (backdrop-filter: blur(10px)) {
        .stTabs [data-baseweb="tab-list"] {
            background: rgba(255, 255, 255, 0.95);
        }
    }
    
    .stTabs [data-baseweb="tab"] {
        background: transparent;
        border-radius: 12px;
        padding: 12px 20px;
        transition: all 0.2s ease;
        border: none;
        font-weight: 500;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);
        color: white !important;
        box-shadow: 0 4px 12px rgba(255, 107, 107, 0.3);
    }
    
    /* Expander - iOS style */
    .streamlit-expanderHeader {
        background: rgba(255, 255, 255, 0.8);
        backdrop-filter: blur(10px);
        -webkit-backdrop-filter: blur(10px);
        border-radius: 12px;
        border: 1px solid rgba(0, 0, 0, 0.1);
        font-weight: 500;
    }
    
    /* Fallback for expander */
    @supports not (backdrop-filter: blur(10px)) {
        .streamlit-expanderHeader {
            background: rgba(255, 255, 255, 0.95);
        }
    }
    
    /* Dataframe styling */
    .stDataFrame {
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
    }
    
    /* Success/Warning/Error - iOS style */
    .stSuccess {
        background: rgba(52, 199, 89, 0.1);
        border: 1px solid rgba(52, 199, 89, 0.3);
        border-radius: 12px;
        color: #34C759;
    }
    
    .stWarning {
        background: rgba(255, 149, 0, 0.1);
        border: 1px solid rgba(255, 149, 0, 0.3);
        border-radius: 12px;
        color: #FF9500;
    }
    
    .stError {
        background: rgba(255, 59, 48, 0.1);
        border: 1px solid rgba(255, 59, 48, 0.3);
        border-radius: 12px;
        color: #FF3B30;
    }
    
    /* Spinner */
    .stSpinner > div {
        color: #FF6B6B;
    }
    
    /* Mobile responsiveness */
    @media (max-width: 768px) {
        .main .block-container {
            padding: 0.5rem;
        }
    }
    
    /* Hide Streamlit branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* Custom scrollbar */
    ::-webkit-scrollbar {
        width: 8px;
    }
    
    ::-webkit-scrollbar-track {
        background: rgba(0, 0, 0, 0.05);
        border-radius: 4px;
    }
    
    ::-webkit-scrollbar-thumb {
        background: rgba(255, 107, 107, 0.3);
        border-radius: 4px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: rgba(255, 107, 107, 0.5);
    }
</style>
""", unsafe_allow_html=True)

# Define the ConvMolFeaturizer
featurizer = ConvMolFeaturizer()

# Function to create iOS-style header
def create_ios_header(title, subtitle=None):
    header_html = f"""
    <div style="background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%); border-radius: 24px; padding: 32px 24px; margin-bottom: 24px; color: white; text-align: center; box-shadow: 0 8px 32px rgba(255, 107, 107, 0.3);">
        <h1 style="margin: 0; font-size: 32px; font-weight: 700; letter-spacing: -0.5px;">{title}</h1>
        {f'<p style="margin: 8px 0 0 0; font-size: 18px; opacity: 0.9; font-weight: 400;">{subtitle}</p>' if subtitle else ''}
    </div>
    """
    st.markdown(header_html, unsafe_allow_html=True)

# Time estimation and formatting functions
def estimate_training_time(n_samples, n_epochs, batch_size):
    """Estimate training time based on dataset characteristics"""
    # Base time per epoch (in seconds)
    base_time_per_epoch = max(n_samples / batch_size * 0.1, 1.0)
    
    # Estimate total time
    estimated_time = base_time_per_epoch * n_epochs
    return max(estimated_time, 10)  # Minimum 10 seconds

def format_time_duration(seconds):
    """Format time duration in a human-readable format"""
    if seconds < 60:
        return f"{int(seconds)}s"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        remaining_seconds = int(seconds % 60)
        return f"{minutes}m {remaining_seconds}s"
    else:
        hours = int(seconds // 3600)
        remaining_minutes = int((seconds % 3600) // 60)
        return f"{hours}h {remaining_minutes}m"

# Enhanced progress tracking
def create_progress_tracker(total_time_estimate, total_epochs):
    """Create an advanced progress tracker"""
    progress_container = st.container()
    
    with progress_container:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Estimated Time", format_time_duration(total_time_estimate))
        
        with col2:
            remaining_time_placeholder = st.empty()
            remaining_time_placeholder.metric("Time Remaining", "Calculating...")
        
        with col3:
            speed_placeholder = st.empty()
            speed_placeholder.metric("Epochs/min", "Starting...")
        
        # Progress bar and status
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        return {
            'progress_bar': progress_bar,
            'status_text': status_text,
            'remaining_time': remaining_time_placeholder,
            'speed': speed_placeholder,
            'start_time': time.time(),
            'total_epochs': total_epochs
        }

def update_progress_tracker(tracker, current_epoch, training_loss, val_loss=None):
    """Update the progress tracker with current status"""
    if current_epoch == 0:
        return
    
    # Calculate progress percentage
    progress_percent = min(current_epoch / tracker['total_epochs'], 1.0)
    
    # Update progress bar
    tracker['progress_bar'].progress(progress_percent)
    
    # Calculate elapsed time and estimated remaining time
    elapsed_time = time.time() - tracker['start_time']
    
    if progress_percent > 0:
        estimated_total_time = elapsed_time / progress_percent
        remaining_time = max(estimated_total_time - elapsed_time, 0)
        
        # Calculate speed (epochs per minute)
        speed = (current_epoch / elapsed_time) * 60 if elapsed_time > 0 else 0
        
        # Update displays
        tracker['remaining_time'].metric("Time Remaining", format_time_duration(remaining_time))
        tracker['speed'].metric("Epochs/min", f"{speed:.1f}")
    
    # Update status
    val_text = f", Val Loss: {val_loss:.4f}" if val_loss is not None else ""
    tracker['status_text'].info(f"üî¨ Epoch {current_epoch}/{tracker['total_epochs']} - Loss: {training_loss:.4f}{val_text}")

# Function to train the model with enhanced tracking
def train_model(df, smiles_column, label_column, batch_size, dropout, nb_epoch, graph_conv_layers, test_size, valid_size, log_transform=False):
    try:
        # Standardize SMILES column before featurization
        df_copy = df.copy()
        standardized_smiles = []
        for smiles in df_copy[smiles_column]:
            try:
                standardized_smile = standardize_smiles(smiles, verbose=False)
                standardized_smiles.append(standardized_smile)
            except Exception as e:
                st.warning(f"Could not standardize SMILES {smiles}, using original: {e}")
                standardized_smiles.append(smiles)
        
        # Featurize the standardized SMILES column
        features = featurizer.featurize(standardized_smiles)

        # Extract the target values from the DataFrame
        targets = df[label_column].tolist()
        
        # Apply log transformation if requested
        if log_transform:
            try:
                # Check for non-positive values
                min_value = min(targets)
                original_range = f"{min_value:.3f} to {max(targets):.3f}"
                offset = 0  # Initialize offset
                if min_value <= 0:
                    offset = abs(min_value) + 1
                    st.warning(f"‚ö†Ô∏è Found non-positive values (min: {min_value}). Adding offset of {offset} before log transformation.")
                    targets = [np.log10(target + offset) for target in targets]
                else:
                    targets = [np.log10(target) for target in targets]
                log_range = f"{min(targets):.3f} to {max(targets):.3f}"
                st.success(f"üìä **Log10 Standardization Applied:**")
                
                # Store the offset for later use in predictions
                st.session_state.log_transform_offset = offset

            except Exception as e:
                st.error(f"‚ùå Error in log transformation: {str(e)}")
                return None, None, None, None, None

        # Create a DeepChem dataset from the features and targets
        dataset = dc.data.NumpyDataset(features, targets)

        # Split the data into training, validation, and test sets
        X_train, X_test, y_train, y_test = train_test_split(dataset.X, dataset.y, test_size=test_size, random_state=42)
        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size/(1 - test_size), random_state=42)

        train_dataset = dc.data.NumpyDataset(X_train, y_train)
        valid_dataset = dc.data.NumpyDataset(X_valid, y_valid)
        test_dataset = dc.data.NumpyDataset(X_test, y_test)

        # Define and train the Graph Convolutional Model
        model_dir = "./trained_graphconv_model"
        if os.path.exists(model_dir):
            shutil.rmtree(model_dir)
        os.makedirs(model_dir)

        n_tasks = 1
        model = GraphConvModel(
            n_tasks, batch_size=batch_size, dropout=dropout, 
            graph_conv_layers=graph_conv_layers, mode='regression', 
            model_dir=model_dir
        )

        # Estimate training time
        estimated_time = estimate_training_time(len(X_train), nb_epoch, batch_size)
        
        if log_transform:
            st.info(f"‚è±Ô∏è Estimated training time: {format_time_duration(estimated_time)} (**Training on Log10 Standardized Scale**)")
        else:
            st.info(f"‚è±Ô∏è Estimated training time: {format_time_duration(estimated_time)}")
        
        # Create progress tracker
        if log_transform:
            st.markdown("### üöÄ GraphConv Training Progress (Log10 Standardized Scale)")
        else:
            st.markdown("### üöÄ GraphConv Training Progress")
        tracker = create_progress_tracker(estimated_time, nb_epoch)

        training_history = {'loss': [], 'val_loss': []}

        for epoch in range(nb_epoch):
            # Train for one epoch
            loss = model.fit(train_dataset, nb_epoch=1)
            training_history['loss'].append(loss)
            
            # Calculate validation loss
            val_loss = model.evaluate(valid_dataset, [dc.metrics.Metric(dc.metrics.mean_squared_error)])
            val_loss_value = val_loss['mean_squared_error']
            training_history['val_loss'].append(val_loss_value)
            
            # Update progress tracker
            update_progress_tracker(tracker, epoch + 1, loss, val_loss_value)

        return model, test_dataset, training_history, model_dir, log_transform

    except Exception as e:
        st.error(f"An error occurred during training: {str(e)}")
        return None, None, None, None, None

# Function to zip a directory
def zipdir(path, ziph):
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(path, '..')))

# Function to plot training history with enhanced styling
def plot_training_history(history, log_transform=False):
    """Create an enhanced training history plot"""
    plt.ioff()  # Turn off interactive mode
    fig, ax = plt.subplots(figsize=(12, 6))
    
    # Set style safely
    try:
        plt.style.use('seaborn-v0_8')
    except OSError:
        try:
            plt.style.use('seaborn')
        except OSError:
            plt.style.use('default')
    
    epochs = range(1, len(history['loss']) + 1)
    
    ax.plot(epochs, history['loss'], 'o-', color='#FF6B6B', linewidth=2, label='Training Loss', markersize=4)
    ax.plot(epochs, history['val_loss'], 'o-', color='#4ECDC4', linewidth=2, label='Validation Loss', markersize=4)
    
    ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')
    
    if log_transform:
        ax.set_ylabel('Loss (Log10 Standardized Scale)', fontsize=12, fontweight='bold')
        ax.set_title('Training History - Log10 Standardized Data', fontsize=14, fontweight='bold', pad=20)
    else:
        ax.set_ylabel('Loss', fontsize=12, fontweight='bold')
        ax.set_title('Training History', fontsize=14, fontweight='bold', pad=20)
    
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_true_vs_pred(y_true, y_pred, log_transform=False):
    """Create an enhanced predicted vs true values plot"""
    plt.ioff()  # Turn off interactive mode
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Set style safely
    try:
        plt.style.use('seaborn-v0_8')
    except OSError:
        try:
            plt.style.use('seaborn')
        except OSError:
            plt.style.use('default')
    
    # Plot 1: Predicted vs True scatter plot
    ax1.scatter(y_true, y_pred, alpha=0.6, color='#FF6B6B', s=60, edgecolors='white', linewidth=1)
    
    # Perfect prediction line
    min_val = min(min(y_true), min(y_pred))
    max_val = max(max(y_true), max(y_pred))
    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, alpha=0.8, label='Perfect Prediction')
    
    # Calculate and display R¬≤
    r2 = r2_score(y_true, y_pred)
    ax1.text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=ax1.transAxes, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
             fontsize=12, verticalalignment='top')
    
    if log_transform:
        ax1.set_xlabel('True Values (Log10 Standardized Scale)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Predicted Values (Log10 Standardized Scale)', fontsize=12, fontweight='bold')
        ax1.set_title('Predicted vs True Values - Log10 Standardized', fontsize=14, fontweight='bold', pad=20)
    else:
        ax1.set_xlabel('True Values', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Predicted Values', fontsize=12, fontweight='bold')
        ax1.set_title('Predicted vs True Values', fontsize=14, fontweight='bold', pad=20)
    
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Residuals plot
    residuals = y_pred - y_true
    ax2.scatter(y_pred, residuals, alpha=0.6, color='#4ECDC4', s=60, edgecolors='white', linewidth=1)
    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.8)
    
    if log_transform:
        ax2.set_xlabel('Predicted Values (Log10 Standardized Scale)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Residuals (Log10 Standardized Scale)', fontsize=12, fontweight='bold')
        ax2.set_title('Residuals Plot - Log10 Standardized Scale', fontsize=14, fontweight='bold', pad=20)
    else:
        ax2.set_xlabel('Predicted Values', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Residuals', fontsize=12, fontweight='bold')
        ax2.set_title('Residuals Plot', fontsize=14, fontweight='bold', pad=20)
    
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

# Deployment functions
def smiles_to_sdf(smiles, sdf_path):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None, "Invalid SMILES string"
    writer = Chem.SDWriter(sdf_path)
    writer.write(mol)
    writer.close()
    return sdf_path, None

def create_dataset(sdf_path):
    try:
        loader = dc.data.SDFLoader(tasks=[], featurizer=dc.feat.ConvMolFeaturizer(), sanitize=True)
        dataset = loader.create_dataset(sdf_path, shard_size=2000)
        return dataset, None
    except Exception as e:
        return None, str(e)

def create_fragment_dataset(sdf_path):
    try:
        loader = dc.data.SDFLoader(tasks=[], featurizer=dc.feat.ConvMolFeaturizer(per_atom_fragmentation=True), sanitize=True)
        frag_dataset = loader.create_dataset(sdf_path, shard_size=5000)
        transformer = dc.trans.FlatteningTransformer(frag_dataset)
        frag_dataset = transformer.transform(frag_dataset)
        return frag_dataset, None
    except Exception as e:
        return None, str(e)

def predict_whole_molecules(model, dataset):
    try:
        pred = model.predict(dataset)
        
        if pred.ndim == 3 and pred.shape[-1] == 2:
            pred = pred[:, :, 1]
        
        pred = pd.DataFrame(pred, index=dataset.ids, columns=["Molecule"])
        return pred, None
    except Exception as e:
        return None, str(e)

def predict_fragment_dataset(model, frag_dataset):
    try:
        pred_frags = model.predict(frag_dataset)
        
        if pred_frags.ndim == 3 and pred_frags.shape[-1] == 2:
            pred_frags = pred_frags[:, :, 1]
        
        pred_frags = pd.DataFrame(pred_frags, index=frag_dataset.ids, columns=["Fragment"])
        return pred_frags, None
    except Exception as e:
        return None, str(e)

def vis_contribs(mol, df):
    try:
        wt = {}
        for n, atom in enumerate(range(mol.GetNumHeavyAtoms())):
            wt[atom] = df.loc[Chem.MolToSmiles(mol), "Contrib"][n]
        return SimilarityMaps.GetSimilarityMapFromWeights(mol, wt)
    except Exception as e:
        st.warning(f"Error in contribution visualization: {str(e)}")
        return None

def inverse_log_transform(value, log_transform_applied=False, offset=0):
    """Apply inverse log transformation if it was applied during training"""
    if log_transform_applied:
        transformed_value = 10 ** value
        # Subtract the offset that was added before log transformation
        return transformed_value - offset
    return value

def format_prediction_value(value, log_transform_applied=False, offset=0):
    """Format prediction value with appropriate information"""
    if log_transform_applied:
        original_value = inverse_log_transform(value, log_transform_applied, offset)
        return f"{original_value:.4f} (log: {value:.4f})"
    return f"{value:.4f}"

# Main function to run the Streamlit app
def main():
    # Create iOS-style header
    create_ios_header("üß¨ GraphConv Regressor", "Graph Convolutional Networks for Molecular Property Prediction")
    
    # Initialize session state
    if 'model_trained' not in st.session_state:
        st.session_state.model_trained = False
    if 'model_dir' not in st.session_state:
        st.session_state.model_dir = None
    if 'loaded_model' not in st.session_state:
        st.session_state.loaded_model = None
    if 'log_transform_applied' not in st.session_state:
        st.session_state.log_transform_applied = False
    if 'log_transform_offset' not in st.session_state:
        st.session_state.log_transform_offset = 0

    # Create tabs for mobile-friendly navigation
    tab1, tab2, tab3, tab4 = st.tabs(["üè† Home", "üî¨ Build Model", "‚öóÔ∏è Predict SMILES", "üìä Batch Predict"])

    with tab1:
        st.markdown("## üéØ Welcome to GraphConv Regressor")
        st.markdown("Leverage the power of Graph Convolutional Networks for accurate molecular property prediction.")
        
        st.markdown("### ‚ú® Key Features")
        st.markdown("""
        - **Graph Convolutional Networks** - Advanced neural networks for molecular graphs
        - **Log10 Standardization Support** - Automatic handling of IC50, Ki, binding affinity data
        - **Real-time Training Progress** - Live updates with time estimation
        - **SMILES Standardization** - Automatic molecular standardization for consistent predictions
        - **Model Export/Import** - Save and load trained models
        - **Batch Processing** - Process multiple molecules at once
        - **Mobile-Optimized** - Works seamlessly on all devices
        """)
        
        st.success("üöÄ Get Started: Use the tabs above to build your model and make predictions!")
        
        # Quick stats
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Model Type", "GraphConv", "Deep learning on molecular graphs")
        with col2:
            st.metric("Prediction Type", "Regression", "Continuous property values")
        with col3:
            st.metric("Data Processing", "Advanced", "SMILES standardization included")

    with tab2:
        st.markdown("## üî¨ Build GraphConv Model")
        st.markdown("Upload your dataset to train a Graph Convolutional Network for molecular property prediction.")
        
        # Sample data section
        with st.expander("üìã Expected Data Format", expanded=False):
            st.write("**Required columns:**")
            st.write("- **SMILES**: Valid SMILES notation for molecules")
            st.write("- **Property**: Numerical values for the target property")
            
            st.write("**Log Transformation:**")
            st.write("- Enable for IC50, Ki, binding affinity data (common in drug discovery)")
            st.write("- Automatically handles non-positive values by adding offset")
            st.write("- Predictions are automatically inverse-transformed to original scale")
            st.write("- **Note:** This creates log10 standardized values for training")
            
            st.write("**Example format:**")
            example_data = pd.DataFrame({
                'SMILES': ['CCO', 'CC(C)O', 'CCCO'],
                'Activity': [5.2, 4.8, 5.1]
            })
            st.dataframe(example_data, use_container_width=True)

        uploaded_file = st.file_uploader(
            "üìÅ Upload Excel file with SMILES and Property Values", 
            type=["xlsx"],
            help="File should contain SMILES column and target property column"
        )

        if uploaded_file is not None:
            try:
                df = pd.read_excel(uploaded_file)
                
                st.markdown(f"### üìä Dataset Overview")
                st.markdown(f"**Shape:** {df.shape[0]} molecules √ó {df.shape[1]} features")
                
                st.dataframe(df, use_container_width=True)

                # Configuration section
                st.markdown("### ‚öôÔ∏è Model Configuration")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    columns = df.columns.tolist()
                    smiles_column = st.selectbox("üß¨ Select SMILES Column", columns)
                    label_column = st.selectbox("üéØ Select Target Property Column", columns)
                    
                    # Log transformation option
                    log_transform = st.checkbox("üìä Apply Log10 Standardization", 
                                               value=False, 
                                               help="Apply log10 standardization to target values (useful for IC50, Ki, binding affinity data)")
                    
                    batch_size = st.number_input("üì¶ Batch Size", min_value=32, max_value=512, value=256, step=32)
                    dropout = st.slider("üõ°Ô∏è Dropout Rate", min_value=0.0, max_value=0.5, value=0.1, step=0.05)

                with col2:
                    nb_epoch = st.number_input("üîÑ Number of Epochs", min_value=10, max_value=500, value=120, step=10)
                    graph_conv_layers = st.text_input("üß† Graph Conv Layers", value="64,64", help="Comma-separated layer sizes")
                    
                    test_size = st.slider("üìä Test Set Size", min_value=0.1, max_value=0.3, value=0.15, step=0.05)
                    valid_size = st.slider("‚úÖ Validation Set Size", min_value=0.1, max_value=0.3, value=0.15, step=0.05)

                # Build model button
                if st.button("üöÄ Build and Train Model", use_container_width=True):
                    if smiles_column and label_column:
                        # Convert graph_conv_layers to list of integers
                        try:
                            graph_conv_layers = [int(layer.strip()) for layer in graph_conv_layers.split(',')]
                        except ValueError:
                            st.error("‚ùå Invalid graph convolution layers format. Please use comma-separated integers.")
                            st.stop()

                        result = train_model(
                            df, smiles_column, label_column, 
                            batch_size, dropout, nb_epoch, graph_conv_layers, test_size, valid_size, log_transform
                        )
                        
                        model, test_dataset, training_history, model_dir, log_transform_applied = result
                        
                        if model is not None:
                            st.session_state.model_trained = True
                            st.session_state.model_dir = model_dir
                            st.session_state.loaded_model = model
                            st.session_state.log_transform_applied = log_transform_applied

                            if log_transform_applied:
                                st.success("üéâ Model training completed with **Log10 Standardization**!")
                                st.info("üìä **Important:** This model was trained on log10-standardized data. All predictions will be automatically converted back to original scale.")
                            else:
                                st.success("üéâ Model training completed!")

                            # Evaluate the model
                            if test_dataset is not None:
                                y_true = np.array(test_dataset.y).ravel()
                                y_pred = model.predict(test_dataset).ravel()

                                # Compute metrics
                                mse = mean_squared_error(y_true, y_pred)
                                rmse = np.sqrt(mse)
                                mae = mean_absolute_error(y_true, y_pred)
                                r2 = r2_score(y_true, y_pred)
                                
                                # Display metrics in standard Streamlit cards
                                st.markdown("### üìä Model Performance Metrics")
                                if log_transform_applied:
                                    st.warning("‚ö†Ô∏è **Note:** Metrics below are calculated in **log10 standardized space** (model's native prediction space)")
                                    st.info("üìä **Interpretation:** Lower loss values in log10 standardized space indicate better model performance")
                                
                                col1, col2, col3, col4 = st.columns(4)
                                
                                with col1:
                                    st.metric("R¬≤ Score", f"{r2:.3f}", f"{'Excellent' if r2 > 0.8 else 'Good' if r2 > 0.6 else 'Fair'}")
                                with col2:
                                    st.metric("RMSE", f"{rmse:.3f}", "Lower is better")
                                with col3:
                                    st.metric("MAE", f"{mae:.3f}", "Mean absolute error")
                                with col4:
                                    st.metric("MSE", f"{mse:.3f}", "Mean squared error")

                                # Enhanced visualizations
                                st.markdown("### üìà Training History")
                                if log_transform_applied:
                                    st.caption("üìä **Training performed on log10 standardized data** - Loss values are in log10 standardized space")
                                fig_history = plot_training_history(training_history, log_transform_applied)
                                st.pyplot(fig_history, use_container_width=True)

                                st.markdown("### üìà Model Prediction Analysis")
                                if log_transform_applied:
                                    st.caption("üìä **Predictions vs True values in log10 standardized space** - Model's native prediction scale")
                                fig_pred = plot_true_vs_pred(y_true, y_pred, log_transform_applied)
                                st.pyplot(fig_pred, use_container_width=True)

                                # Provide download link for the trained model
                                zipf = zipfile.ZipFile('trained_graphconv_model.zip', 'w', zipfile.ZIP_DEFLATED)
                                zipdir(model_dir, zipf)
                                zipf.close()

                                with open('trained_graphconv_model.zip', 'rb') as f:
                                    st.download_button(
                                        label="üì• Download Trained Model",
                                        data=f,
                                        file_name='trained_graphconv_model.zip',
                                        mime='application/zip',
                                        use_container_width=True
                                    )
                            else:
                                st.error("‚ùå Test dataset is empty. Training may have failed.")
                        else:
                            st.error("‚ùå Model training failed. Please check your data and try again.")
                    else:
                        st.error("‚ùå Please select both the SMILES and target property columns.")

            except Exception as e:
                st.error(f"‚ùå Error reading file: {str(e)}")

    with tab3:
        st.markdown("## ‚öóÔ∏è Single Molecule Prediction")
        st.markdown("Enter a SMILES string to predict molecular properties.")

        # Check if model is trained
        if not st.session_state.model_trained:
            st.markdown("### üìÅ Load Trained Model")
            uploaded_zip = st.file_uploader("Upload model zip file", type=['zip'])
            
            if uploaded_zip is not None:
                try:
                    # Extract the zip file
                    with zipfile.ZipFile(uploaded_zip, 'r') as zip_ref:
                        model_dir = 'temp_model_dir'
                        if os.path.exists(model_dir):
                            shutil.rmtree(model_dir)
                        os.makedirs(model_dir)
                        zip_ref.extractall(model_dir)
                    
                    # Load the model
                    n_tasks = 1
                    model = GraphConvModel(n_tasks, model_dir=model_dir)
                    model.restore()
                    
                    st.session_state.loaded_model = model
                    st.session_state.model_dir = model_dir
                    st.session_state.model_trained = True
                    
                    st.success("‚úÖ Model loaded successfully!")
                    st.warning("‚ö†Ô∏è **Important:** If this model was trained with log transformation, make sure to manually set the log transformation settings. The system cannot automatically detect transformation parameters from saved models.")
                        
                except Exception as e:
                    st.error(f"‚ùå Error loading model: {str(e)}")
                    st.stop()

        # Show model training status
        if st.session_state.model_trained and st.session_state.loaded_model:
            # Add manual log transformation controls for loaded models
            with st.expander("üîß Log Transformation Settings (for loaded models)", expanded=False):
                st.markdown("**Configure log transformation settings if your loaded model was trained with log transformation:**")
                
                col1, col2 = st.columns(2)
                with col1:
                    manual_log_transform = st.checkbox("Model uses log transformation", 
                                                     value=st.session_state.get('log_transform_applied', False),
                                                     key='manual_log_transform')
                with col2:
                    manual_offset = st.number_input("Log transformation offset", 
                                                  value=float(st.session_state.get('log_transform_offset', 0.0)),
                                                  min_value=0.0,
                                                  max_value=1000.0,
                                                  step=0.1,
                                                  key='manual_offset',
                                                  help="Offset added before log transformation (0 if no offset was used)")
                
                if st.button("Update Settings"):
                    st.session_state.log_transform_applied = manual_log_transform
                    st.session_state.log_transform_offset = manual_offset
                    st.success("‚úÖ Log transformation settings updated!")
            
            if st.session_state.get('log_transform_applied', False):
                offset = st.session_state.get('log_transform_offset', 0)
                if offset > 0:
                    st.info(f"üî¨ **Model Status:** Trained on log10-transformed activity values with offset {offset:.1f}. Predictions will be automatically converted back to original scale.")
                else:
                    st.info("üî¨ **Model Status:** Trained on log10-transformed activity values. Predictions will be automatically converted back to original scale.")
            else:
                st.info("üî¨ **Model Status:** Trained on original scale activity values.")

        # SMILES input
        smiles_input = st.text_input(
            "üß¨ Enter SMILES string", 
            placeholder="e.g., CCO (ethanol)",
            help="Enter a valid SMILES notation for the molecule you want to predict"
        )

        if st.button("üîÆ Predict Property", use_container_width=True):
            if smiles_input:
                with st.spinner("üß¨ Analyzing molecule..."):
                    try:
                        # Standardize the SMILES input
                        try:
                            standardized_smiles = standardize_smiles(smiles_input, verbose=False)
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Could not standardize SMILES, using original: {e}")
                            standardized_smiles = smiles_input
                        
                        # Convert SMILES to SDF
                        sdf_path = "input_molecule.sdf"
                        sdf_path, error = smiles_to_sdf(standardized_smiles, sdf_path)

                        if error:
                            st.error(f"‚ùå Error in SMILES to SDF conversion: {error}")
                        else:
                            # Create dataset
                            dataset, error = create_dataset(sdf_path)

                            if error:
                                st.error(f"‚ùå Error in dataset creation: {error}")
                            else:
                                # Make predictions for whole molecules
                                predictions_whole, error = predict_whole_molecules(st.session_state.loaded_model, dataset)

                                if error:
                                    st.error(f"‚ùå Error in predicting whole molecules: {error}")
                                else:
                                    # Generate molecule and display results
                                    mol = Chem.MolFromSmiles(smiles_input)

                                    if mol:
                                        # Results in standard layout
                                        col1, col2 = st.columns([1, 1])
                                        
                                        with col1:
                                            st.markdown("### üéØ Prediction Results")
                                            predicted_value = predictions_whole['Molecule'].iloc[0]
                                            
                                            # Handle log transformation display
                                            if st.session_state.get('log_transform_applied', False):
                                                offset = st.session_state.get('log_transform_offset', 0)
                                                original_value = inverse_log_transform(predicted_value, True, offset)
                                                st.metric("Final Prediction (Original Scale)", f"{original_value:.4f}")
                                                st.caption(f"üìä Raw model output (log10): {predicted_value:.4f}")
                                                if offset > 0:
                                                    st.caption(f"‚úÖ Converted from log10 and removed offset ({offset:.1f})")
                                                else:
                                                    st.caption("‚úÖ Automatically converted from log10 back to original scale")
                                            else:
                                                st.metric("Predicted Property Value", f"{predicted_value:.4f}")
                                            
                                            st.write(f"**Input SMILES:** {smiles_input}")
                                            st.write(f"**Standardized:** {standardized_smiles}")
                                        
                                        with col2:
                                            st.markdown("### üß™ Molecular Structure")
                                            img = Chem.Draw.MolToImage(mol, size=(300, 300))
                                            st.image(img, caption='2D Molecular Structure', use_column_width=True)
                                    else:
                                        st.error("‚ùå Unable to generate molecule from input SMILES.")

                                    # Clean up
                                    if os.path.exists(sdf_path):
                                        os.remove(sdf_path)

                    except Exception as e:
                        st.error(f"‚ùå Error: {str(e)}")

    with tab4:
        st.markdown("## üìä Batch Prediction")
        st.markdown("Upload an Excel file with multiple SMILES to predict properties.")
        
        # Check if model is trained
        if not st.session_state.model_trained:
            st.warning("‚ö†Ô∏è Please load a trained model first in the 'Predict SMILES' tab.")
            st.stop()
        
        # Show model training status for batch prediction
        if st.session_state.get('log_transform_applied', False):
            st.info("üî¨ **Batch Processing Mode:** Model trained on log10-transformed values. All predictions will be automatically converted back to original scale.")
        else:
            st.info("üî¨ **Batch Processing Mode:** Model trained on original scale values.")

        uploaded_pred_file = st.file_uploader(
            "üìÅ Upload Excel file with SMILES for batch prediction", 
            type=["xlsx"],
            key="batch_prediction_file"
        )

        if uploaded_pred_file is not None:
            try:
                pred_df = pd.read_excel(uploaded_pred_file)
                
                st.markdown("### üìä Prediction Dataset")
                st.markdown(f"**Shape:** {pred_df.shape[0]} molecules √ó {pred_df.shape[1]} columns")
                
                st.dataframe(pred_df, use_container_width=True)

                # Assume the column is named 'SMILES' or let user select
                if 'SMILES' in pred_df.columns:
                    smiles_col = 'SMILES'
                elif 'Smile' in pred_df.columns:
                    smiles_col = 'Smile'
                else:
                    pred_col_names = pred_df.columns.tolist()
                    smiles_col = st.selectbox("üß¨ Select SMILES Column", pred_col_names, key='batch_smiles_column')

                if st.button("üöÄ Run Batch Prediction", use_container_width=True):
                    predictions = []
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    results_container = st.container()
                    
                    for idx, row in pred_df.iterrows():
                        smiles = row[smiles_col]
                        progress = (idx + 1) / len(pred_df)
                        progress_bar.progress(progress)
                        status_text.info(f"üîÆ Predicting molecule {idx + 1}/{len(pred_df)}: {smiles}")
                        
                        try:
                            # Standardize the SMILES input
                            try:
                                standardized_smiles = standardize_smiles(smiles, verbose=False)
                            except Exception as e:
                                st.warning(f"‚ö†Ô∏è Could not standardize SMILES {smiles}, using original: {e}")
                                standardized_smiles = smiles
                            
                            # Convert SMILES to SDF
                            sdf_path = f"input_molecule_{idx}.sdf"
                            sdf_path, error = smiles_to_sdf(standardized_smiles, sdf_path)

                            if error:
                                predictions.append("Failed")
                                continue

                            # Create dataset and predict
                            dataset, error = create_dataset(sdf_path)
                            if error:
                                predictions.append("Failed")
                                continue

                            predictions_whole, error = predict_whole_molecules(st.session_state.loaded_model, dataset)
                            if error:
                                predictions.append("Failed")
                                continue

                            predicted_value = predictions_whole['Molecule'].iloc[0]
                            
                            # Handle log transformation for display
                            if st.session_state.get('log_transform_applied', False):
                                offset = st.session_state.get('log_transform_offset', 0)
                                original_value = inverse_log_transform(predicted_value, True, offset)
                                display_value = original_value
                                predictions.append(display_value)
                            else:
                                display_value = predicted_value
                                predictions.append(predicted_value)
                            
                            # Show individual results
                            with results_container:
                                col1, col2, col3 = st.columns([2, 1, 1])
                                with col1:
                                    st.write(f"**{idx + 1}.** {smiles}")
                                    if smiles != standardized_smiles:
                                        st.caption(f"Standardized: {standardized_smiles}")
                                with col2:
                                    mol = Chem.MolFromSmiles(standardized_smiles)
                                    if mol:
                                        img = Chem.Draw.MolToImage(mol, size=(100, 100))
                                        st.image(img, width=100)
                                with col3:
                                    if st.session_state.get('log_transform_applied', False):
                                        offset = st.session_state.get('log_transform_offset', 0)
                                        st.metric("Final Prediction", f"{display_value:.3f}")
                                        st.caption(f"Log10: {predicted_value:.3f}")
                                        if offset > 0:
                                            st.caption(f"‚úÖ Converted (offset: {offset:.1f})")
                                        else:
                                            st.caption("‚úÖ Converted to original scale")
                                    else:
                                        st.metric("Prediction", f"{predicted_value:.3f}")

                            # Clean up
                            if os.path.exists(sdf_path):
                                os.remove(sdf_path)

                        except Exception as e:
                            predictions.append("Failed")
                            st.warning(f"‚ö†Ô∏è Failed to process {smiles}: {str(e)}")
                    
                    # Final results
                    pred_df['Predicted_Property'] = predictions
                    
                    st.markdown("### üéä Batch Prediction Complete!")
                    st.success(f"""
                    **Results Summary:**
                    - **Total Predictions:** {len(predictions)}
                    - **Successful:** {sum(1 for p in predictions if p != 'Failed')}
                    - **Failed:** {sum(1 for p in predictions if p == 'Failed')}
                    """)
                    
                    st.dataframe(pred_df, use_container_width=True)
                    
                    # Download results
                    csv_data = pred_df.to_csv(index=False)
                    st.download_button(
                        "üì• Download Results as CSV",
                        data=csv_data,
                        file_name="batch_predictions_graphconv.csv",
                        mime="text/csv",
                        use_container_width=True
                    )

            except Exception as e:
                st.error(f"‚ùå Error processing file: {str(e)}")


if __name__ == "__main__":
    main()
